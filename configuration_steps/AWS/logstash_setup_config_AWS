Step 1: Update and Upgrade Packages
    - sudo apt update
    - sudo apt upgrade -y
Step 2: Step 2: Install Java (Prerequisite)
    - sudo apt install -y openjdk-11-jdk
    - java -version
Step 3: Add the Elastic Repository
    - Install the Elastic GPG key: wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
    - Add the Elastic repository: echo "deb https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list
Step 4: Install Logstash
    - sudo apt update
    - sudo apt install logstash -y
Step 5: Configure Logstash
    - cd /etc/logstash/conf.d/
    - sudo touch kafka-to-elasticsearch.conf
    - sudo nano /etc/logstash/conf.d/kafka-to-elasticsearch.conf
    - Add the following configuration:
        input {
          kafka {
            bootstrap_servers => "13.203.157.163:9092"    # Replace with your Kafka broker address
            topics => [
        	  "click-processed-events",
              "view-processed-events",
              "purchase-processed-events",
              "search-processed-events",
              "error-processed-events"
        	 ]        								 # Replace with your Kafka topic
            group_id => "logstash-consumers-group"   # Optional: consumer group ID
            codec => "json"                          # Specify the codec if data is in JSON format
        	decorate_events => true  			     # Ensures metadata like `[@metadata][kafka][topic]` is added
          }
        }

        filter {
        }

        output {
          if [@metadata][kafka][topic] == "click-processed-events" {
            elasticsearch {
              hosts => ["http://15.206.212.238:9200"]
              index => "click-processed-events-index"
              document_id => "%{[eventId]}"
            }
          } else if [@metadata][kafka][topic] == "view-processed-events" {
            elasticsearch {
              hosts => ["http://15.206.212.238:9200"]
              index => "view-processed-events-index"
              document_id => "%{[eventId]}"
            }
          } else if [@metadata][kafka][topic] == "purchase-processed-events" {
            elasticsearch {
              hosts => ["http://15.206.212.238:9200"]
              index => "purchase-processed-events-index"
              document_id => "%{[eventId]}"
            }
          } else if [@metadata][kafka][topic] == "search-processed-events" {
            elasticsearch {
              hosts => ["http://15.206.212.238:9200"]
              index => "search-processed-events-index"
              document_id => "%{[eventId]}"
            }
          } else if [@metadata][kafka][topic] == "error-processed-events" {
            elasticsearch {
              hosts => ["http://15.206.212.238:9200"]
              index => "error-processed-events-index"
              document_id => "%{[eventId]}"
            }
          } else {
            stdout { codec => rubydebug } # Debug output for unmatched events
          }
        }

        #output {
        #  elasticsearch {
        #    hosts => ["http://15.206.212.238:9200"]
        #    user => "elastic"
        #    password => "Z9Ei3Etp-LYD4Ll*IsGs"
        #    ssl => true
        #    cacert => "C:/elastic-stack/elasticsearch-8.17.0/config/certs/http_ca.crt" # Path to the CA certificate
        #	index => "processed-events-index"     # Replace with your desired index name
        # 	document_id => "%{[eventId]}"         # Unique ID for deduplication (adjust field if needed)
        #  }
        #
        #  stdout {
        #    codec => rubydebug                        # Optional: Debug output to console
        #  }
        #}


Backup Configuration: Add the following configuration

        input {
          kafka {
            bootstrap_servers => "13.203.157.163:9092" # Replace with your Kafka broker address
            topics => ["processed-events"]        # Replace with your Kafka topic
        #    group_id => "logstash-group"         # Optional: consumer group ID
            codec => "json"                       # Specify the codec if data is in JSON format
          }
        }

        filter {
          # Optional: Add any data transformation or filtering logic here
          # Example:
          # mutate {
          #   rename => { "field1" => "new_field1" }
          # }
        }

        output {
          elasticsearch {
            hosts => ["http://15.206.212.238:9200"]    # Replace with your Elasticsearch URL
            index => "processed-events-index"     # Replace with your desired index name
                document_id => "%{[eventId]}"         # Unique ID for deduplication
          }

         # Optional: Debugging output
          stdout {
            codec => rubydebug
          }
        }


        output {
          elasticsearch {
            hosts => ["15.206.212.238:9200"]    # Replace with your Elasticsearch URL
            index => "processed-events-index"     # Replace with your desired index name
        	document_id => "%{[eventId]}"         # Unique ID for deduplication
          }

         # Optional: Debugging output
          stdout {
            codec => rubydebug
          }
        }
Step 6 â€”  Add Swap Space :
             1. Create a swap file: Create a swap file: sudo fallocate -l 1G /swapfile
             2. Set the correct permissions: sudo chmod 600 /swapfile
             3. Set up the swap space:  sudo mkswap /swapfile
             4. Enable the swap file: sudo swapon /swapfile
             5. Verify the swap space: swapon --show
                                       free -h
             6. Make the swap space permanent by adding it to /etc/fstab:
                 - echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab

Step 7: Start and Enable Logstash Service
    - sudo systemctl start logstash
    - sudo systemctl enable logstash

Step 8 : Logs
    - sudo journalctl -u logstash -f

step 8 : stop logstash & Restart logstash
    - sudo systemctl stop logstash
    - sudo systemctl restart logstash

Step 10 : server status
    - sudo systemctl status logstash